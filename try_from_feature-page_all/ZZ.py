import requests
from bs4 import BeautifulSoup
import datetime
import random
import re
import time

# è®¾ç½®éšæœºç§å­
random.seed(int(datetime.datetime.now().timestamp()))

# è®¾ç½®è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰
request_interval = 2  # æ¯æ¬¡è¯·æ±‚åç­‰å¾…2ç§’

# è®¾ç½®æœ€å¤§æ‰§è¡Œæ—¶é•¿ï¼ˆç§’ï¼‰
max_execution_time = 60  # è„šæœ¬æœ€é•¿è¿è¡Œæ—¶é—´ä¸º60ç§’

def get_links(article_url):
    try:
        html_add = "https://baike.sogou.com" + article_url
        response = requests.get(html_add, timeout=10)
        if response.status_code == 200:     # çŠ¶æ€ç 200çš„å…·ä½“å«ä¹‰ï¼šè¿™æ˜¯æœ€æ™®éçš„æˆåŠŸçŠ¶æ€ç ï¼Œæ„å‘³ç€è¯·æ±‚å·²æˆåŠŸï¼Œ
                                            # ä¸”æ‰€è¯·æ±‚çš„æ•°æ®åŒ…å«åœ¨å“åº”ä½“ä¸­ã€‚å½“æœåŠ¡å™¨è¿”å›æ­¤çŠ¶æ€ç æ—¶,è¡¨æ˜å®¢æˆ·ç«¯çš„è¯·æ±‚è¢«æ­£ç¡®æ¥æ”¶ã€ç†è§£å’Œå¤„ç†ã€‚
            bsobj = BeautifulSoup(response.text, 'html.parser')
            print(bsobj.text,'\n', bsobj.title)




            

            # ç¨‹åºæ‰§è¡Œåˆ°è¿™é‡Œï¼Œç»ˆç«¯æ˜¾ç¤ºï¼šâ€œé¡µé¢ FORBIDDEN ğŸ˜¯â€ï¼Œ åé¢æ— æ³•è§£æç½‘é¡µäº†ï¼ 







            # æŸ¥æ‰¾æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„é“¾æ¥
            # å‡è®¾é“¾æ¥åœ¨å…·æœ‰ç‰¹å®šç±»åçš„ <a> æ ‡ç­¾ä¸­ï¼Œä¾‹å¦‚ class="lemma-title"
            # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´é€‰æ‹©å™¨
            # links = bsobj.find_all("a", href=re.compile("^(/sogou/.*|/v/.*)$"))   # è¿™ä¸ªæ­£åˆ™è¡¨è¾¾å¼é”™è¯¯ï¼Œæ— æ³•æ‰¾åˆ°ä¸œè¥¿ï¼Œ  å…·ä½“å•¥æ˜¯æ­£ç¡®ï¼Œè¦çœ‹æœç‹—ç™¾ç§‘çš„URLåœ°å€æ‰çŸ¥é“ã€‚
            # links = bsobj.find_all("a", href=re.compile("^/v\d+\.htm$"))
            # pattern = r"^/lemma/ShowInnerLink\.htm\?lemmaId=\d+(&.*)?$"
            pattern = r"^/lemma/ShowInnerLink\.htm\?lemmaId="
            links = bsobj.find_all("a", class_='ed_inner_link', href=re.compile(pattern))



            if not links:
                print(f"No links found for {article_url}")
                return []
            
            # è¿‡æ»¤å‡ºæœ‰æ•ˆçš„é“¾æ¥
            valid_links = [link for link in links if 'href' in link.attrs]
            return valid_links
        else:
            print(f"Failed to retrieve page {article_url}, status code: {response.status_code}")
            return []
    except Exception as e:
        print(f"Error encountered while processing {article_url}: {e}")
        return []

# è®°å½•å¼€å§‹æ—¶é—´
start_time = time.time()

# åˆå§‹æ–‡ç« é“¾æ¥ï¼Œä¾‹å¦‚â€œç§‘å­¦â€è¯æ¡
# links = get_links("/v/%E7%A7%91%E5%AD%A6")    # ä¸è¦åˆå§‹æ–‡ç« äº†ï¼Œå› ä¸ºæ‰¾ä¸åˆ°ä¸œè¥¿çš„ï¼Œä¸å¦‚ä¸ºç©º
links = get_links("/v42423.htm")   # ä»â€œæ•°å­¦â€è¿™ä¸ªè¯æ¡å¼€å§‹      


while len(links) > 0:
    current_time = time.time()
    if (current_time - start_time) > max_execution_time:
        print("Maximum execution time reached. Exiting.")
        break
    
    new_article = links[random.randint(0, len(links)-1)].attrs['href']
    print(new_article)
    
    # åœ¨ä¸‹ä¸€æ¬¡è¯·æ±‚å‰æš‚åœæŒ‡å®šç§’æ•°
    time.sleep(request_interval)
    
    links = get_links(new_article)